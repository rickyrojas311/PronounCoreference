{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricky\\miniforge3\\envs\\ds125\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "06/02/2024 19:49:11 - INFO - \t missing_keys: []\n",
      "06/02/2024 19:49:11 - INFO - \t unexpected_keys: []\n",
      "06/02/2024 19:49:11 - INFO - \t mismatched_keys: []\n",
      "06/02/2024 19:49:11 - INFO - \t error_msgs: []\n",
      "06/02/2024 19:49:11 - INFO - \t Model Parameters: 90.5M, Transformer: 82.1M, Coref head: 8.4M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "import mne\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import syllapy\n",
    "from fastcoref import spacy_component\n",
    "from mne.stats import permutation_cluster_test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.add_pipe(\"fastcoref\")\n",
    "\n",
    "mne.set_log_level(verbose=False)\n",
    "\n",
    "\n",
    "def read_meg(meg_path: str, event_path: str):\n",
    "    \"\"\"\n",
    "    Read and preprocess MEG data from the given path, clean and filter the data,\n",
    "    and return it along with the event DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    meg_path (str): The file path to the MEG data.\n",
    "    event_path (str): The file path to the event data.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the preprocessed raw MEG data and the cleaned event DataFrame.\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_ctf(meg_path, preload=False)\n",
    "    raw.pick(picks=['mag'])\n",
    "    raw.resample(100)\n",
    "    raw.load_data()\n",
    "    raw.filter(0.1, 30, method='iir')\n",
    "\n",
    "    df = pd.read_csv(event_path, delimiter='\\t')\n",
    "    df_crop = df[df['type'].str.contains('word_onset', na=False)]\n",
    "    df_crop = df_crop.query(\"value != 'sp'\")\n",
    "\n",
    "    return raw, df_crop\n",
    "\n",
    "\n",
    "def session_text(root_dir: str, session: str) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate text from files matching a specific session pattern in the given directory.\n",
    "\n",
    "    Parameters:\n",
    "    root_dir (str): The root directory containing the stimuli files.\n",
    "    session (str): The session identifier to match files.\n",
    "\n",
    "    Returns:\n",
    "    str: The concatenated text from all matching files.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    pattern = re.compile(f'{session}_\\\\d\\\\.txt')\n",
    "    stimuli_dir = os.path.join(root_dir, \"stimuli\")\n",
    "\n",
    "    for filename in os.listdir(stimuli_dir):\n",
    "        if pattern.match(filename):\n",
    "            file_path = os.path.join(stimuli_dir, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                full_text += file.read().replace(\"\\n\", \" \")\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def story_text(root_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Read and concatenate full story from the specified directory.\n",
    "\n",
    "    Parameters:\n",
    "    root_dir (str): The root directory containing the text files.\n",
    "\n",
    "    Returns:\n",
    "    str: The concatenated text from all the text files in the directory.\n",
    "    \"\"\"\n",
    "    full_text = \"\"\n",
    "    stimuli_dir = os.path.join(root_dir, \"stimuli\")\n",
    "\n",
    "    for filename in os.listdir(stimuli_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(stimuli_dir, filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                full_text += file.read().replace(\"\\n\", \" \")\n",
    "\n",
    "    return full_text\n",
    "\n",
    "\n",
    "def has_accented_characters(token) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if the given token has any accented characters.\n",
    "\n",
    "    Parameters:\n",
    "    token (spacy.tokens.Token): The token to be checked.\n",
    "\n",
    "    Returns:\n",
    "    bool: True if the token has accented characters, False otherwise.\n",
    "    \"\"\"\n",
    "    return any(ord(char) > 127 for char in token.text)\n",
    "\n",
    "\n",
    "def find_accent(token) -> int:\n",
    "    \"\"\"\n",
    "    Finds the index of the first accented character in the token.\n",
    "\n",
    "    Parameters:\n",
    "    token (spacy.tokens.Token): The token to be checked.\n",
    "\n",
    "    Returns:\n",
    "    int: The index of the first accented character, or -1 if none are found.\n",
    "    \"\"\"\n",
    "    for idx, char in enumerate(token.text):\n",
    "        if ord(char) > 127:\n",
    "            return idx\n",
    "    return -1\n",
    "\n",
    "\n",
    "def pos_tagger(doc: spacy.tokens.Doc, ses_id, pt_id) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Tags parts of speech in the given text. Note: This function does not work \n",
    "    on words with more than one accent.\n",
    "\n",
    "    Parameters:\n",
    "    doc (spacy.tokens.Doc): The spacy Doc object to be processed.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing two lists - parts of speech tags and corresponding tokens.\n",
    "    \"\"\"\n",
    "    pos = []\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if ses_id == \"01\" and pt_id == \"03\":\n",
    "            if token.i < 10:\n",
    "                continue\n",
    "        if token.text.strip() and not token.is_punct:\n",
    "            if has_accented_characters(token):\n",
    "                text = token.text[find_accent(token) + 1:]\n",
    "                if text:\n",
    "                    pos.append(token.pos_)\n",
    "                    tokens.append(text)\n",
    "            elif token.text.lower() in [\"n't\", \"'ll\", \"'ve\", \"'m\", \"'d\", \"'t\", \"'s\", \"'re\"]:\n",
    "                tokens[-1] += token.text\n",
    "            else:\n",
    "                pos.append(token.pos_)\n",
    "                tokens.append(token.text)\n",
    "\n",
    "    return pos, tokens\n",
    "\n",
    "\n",
    "def get_reduced_tokens(spans: List[spacy.tokens.Span]) -> List[spacy.tokens.Token]:\n",
    "    \"\"\"\n",
    "    Reduce tokens in each span to a single representative token based on part of speech tags.\n",
    "\n",
    "    Parameters:\n",
    "    spans (List[spacy.tokens.Span]): A list of spacy token spans.\n",
    "\n",
    "    Returns:\n",
    "    List[spacy.tokens.Token]: A list of reduced tokens.\n",
    "    \"\"\"\n",
    "    reduced_tokens = []\n",
    "\n",
    "    for span in spans:\n",
    "        pnoun = False\n",
    "        reduced_token = None\n",
    "        nouns = []\n",
    "\n",
    "        for token in span:\n",
    "            if token.pos_ in ['PROPN', 'PRON', 'NOUN']:\n",
    "                if token.pos_ == \"PROPN\":\n",
    "                    pnoun = True\n",
    "                    reduced_token = token\n",
    "                elif not pnoun:\n",
    "                    nouns.append(token)\n",
    "\n",
    "        if pnoun:\n",
    "            reduced_tokens.append(reduced_token)\n",
    "        elif nouns:\n",
    "            reduced_tokens.append(random.choice(nouns))\n",
    "\n",
    "    return reduced_tokens\n",
    "\n",
    "\n",
    "def get_head(reduced_spans) -> str:\n",
    "    \"\"\"\n",
    "    Determine the head token from the given spans based on their part of speech tags.\n",
    "\n",
    "    Parameters:\n",
    "    reduced_spans (list): A list of spacy tokens.\n",
    "\n",
    "    Returns:\n",
    "    str: The head token with the highest frequency, or an empty string if no head token is found.\n",
    "    \"\"\"\n",
    "    pnoun = False\n",
    "    head_token = {}\n",
    "\n",
    "    for token in reduced_spans:\n",
    "        if token.pos_ in ['PROPN', 'PRON', 'NOUN']:\n",
    "            if token.pos_ == \"PROPN\":\n",
    "                pnoun = True\n",
    "                head_token[token.text] = head_token.get(token.text, 0) + 1\n",
    "            elif token.pos_ == \"NOUN\" and not pnoun:\n",
    "                head_token[token.text] = head_token.get(token.text, 0) + 1\n",
    "\n",
    "    if not head_token:\n",
    "        return \"\"\n",
    "\n",
    "    return max(head_token, key=head_token.get)\n",
    "\n",
    "\n",
    "def coref_tagger(doc: spacy.tokens.Doc, ses_id, pt_id) -> List[str]:\n",
    "    \"\"\"\n",
    "    Processes the given text to resolve coreferences. Note: This function \n",
    "    does not work on words with more than one accent.\n",
    "\n",
    "    Parameters:\n",
    "    doc (spacy.tokens.Doc): The spacy Doc object to be processed.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of resolved tokens or None for unresolved tokens.\n",
    "    \"\"\"\n",
    "    resolved = []\n",
    "    cluster_dict = {}\n",
    "\n",
    "    # Extract coreference clusters\n",
    "    clusters = doc._.coref_clusters\n",
    "    for cluster in clusters:\n",
    "        spans = [doc.char_span(span[0], span[1]) for span in cluster]\n",
    "        reduced_tokens = get_reduced_tokens(spans)\n",
    "        cluster_head = get_head(reduced_tokens)\n",
    "        for token in reduced_tokens:\n",
    "            cluster_dict[token] = cluster_head\n",
    "\n",
    "    for token in doc:\n",
    "        if ses_id == \"01\" and pt_id == \"03\":\n",
    "            if token.i < 10:\n",
    "                continue\n",
    "        if token.text.strip() and not token.is_punct and \\\n",
    "                token.text.lower() not in [\"n't\", \"'ll\", \"'ve\", \"'m\", \"'d\", \"'t\", \"'s\", \"'re\"]:\n",
    "            if has_accented_characters(token):\n",
    "                if ord(token.text[-1]) > 127:\n",
    "                    continue\n",
    "            if token in cluster_dict and cluster_dict[token] != \"\":\n",
    "                resolved.append(cluster_dict[token])\n",
    "            else:\n",
    "                resolved.append(None)\n",
    "\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def create_epochs(df: pd.DataFrame, raw: mne.io.Raw) -> mne.Epochs:\n",
    "    \"\"\"\n",
    "    Create epochs from MEG data using event information from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing event information with 'onset' column.\n",
    "    raw (mne.io.Raw): The raw MEG data.\n",
    "\n",
    "    Returns:\n",
    "    mne.Epochs: The created epochs.\n",
    "    \"\"\"\n",
    "    word_samples = np.array(df['onset'] * raw.info['sfreq'], dtype='int')\n",
    "    n_words = len(word_samples)\n",
    "\n",
    "    word_events = np.zeros((n_words, 3), dtype='int')\n",
    "    word_events[:, 0] = word_samples\n",
    "\n",
    "    epochs = mne.Epochs(raw, word_events, tmin=-2.0, tmax=2.0,\n",
    "                        baseline=(-2.0, 2.0), preload=False, metadata=df)\n",
    "    return epochs\n",
    "\n",
    "\n",
    "def run_LR_model(epochs: mne.Epochs, labels: pd.DataFrame, pipeline) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run a Logistic Regression model on the MEG epochs data with cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    epochs (mne.Epochs): The epochs containing MEG data.\n",
    "    labels (pd.DataFrame): The DataFrame containing labels for classification.\n",
    "    pipeline (sklearn.pipeline.Pipeline): The scikit-learn pipeline for the model.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing AUC scores for each label.\n",
    "    \"\"\"\n",
    "    df_scores = pd.DataFrame()\n",
    "\n",
    "    for column in labels.columns:\n",
    "        y = labels[column].to_numpy().ravel()\n",
    "        auc_score = []\n",
    "\n",
    "        for i in range(epochs.get_data().shape[2]):\n",
    "            X = epochs.get_data()[:, :, i]\n",
    "            skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "            scores = []\n",
    "\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                score = roc_auc_score(\n",
    "                    y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "                scores.append(score)\n",
    "\n",
    "            auc_score.append(np.mean(scores))\n",
    "\n",
    "        df_scores[column] = auc_score\n",
    "\n",
    "    return df_scores\n",
    "\n",
    "\n",
    "def analysis_2(root_dir: str, save_dir: str, pat_id: str):\n",
    "    \"\"\"\n",
    "    Perform Analysis 2 on the given patient data.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str): The root directory path where the patient data is stored.\n",
    "        save_dir (str): The directory path where the analysis results will be saved.\n",
    "        pat_id (str): The patient ID.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    ses_ids = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "    characters = ['Holmes', 'Watson', 'Doctor', 'McCarthy', 'Clair', 'Doran', \n",
    "                  'Turner', 'Sutherland', 'Simon', 'Ryder', 'Frank', 'Hatherley', \n",
    "                  'Stoner', 'Adler', 'Wilson', 'Ferguson', 'Angel', 'Lestrade']\n",
    "    pronouns = ['I', 'YOU', 'HE', 'MY', 'HIS', 'ME', 'YOUR', 'SHE', 'HIM', 'HER']\n",
    "\n",
    "    for ses_id in ses_ids:\n",
    "        meg_path = f'{root_dir}/sub-0{pat_id}/ses-0{ses_id}/meg/sub-0{pat_id}_ses-0{ses_id}_task-compr_meg.ds'\n",
    "        event_path = f'{root_dir}/sub-0{pat_id}/ses-0{ses_id}/meg/sub-0{pat_id}_ses-0{ses_id}_task-compr_events.tsv'\n",
    "\n",
    "        # raw, df = read_meg(meg_path, event_path)\n",
    "        text = session_text(root_dir, ses_id)\n",
    "\n",
    "        print(f\"Processing session {ses_id} for patient {pat_id}\")\n",
    "\n",
    "        epochs = mne.read_epochs(f\"{root_dir}/sub_0{pat_id}/ses_0{ses_id}/clean-epo.fif\", preload=False)\n",
    "        df = epochs.metadata\n",
    "\n",
    "        doc = nlp(text)\n",
    "        df['POS'], _ = pos_tagger(doc, ses_id, pat_id)\n",
    "        df['coref'] = coref_tagger(doc, ses_id, pat_id)\n",
    "        df[\"syllables\"] = df[\"value\"].apply(syllapy.count)\n",
    "\n",
    "        mask = df[\"POS\"].isin([\"PROPN\", \"PRON\", \"NOUN\"])\n",
    "        noun_epochs = epochs[mask]\n",
    "        valid_epochs = noun_epochs[pd.notnull(noun_epochs.metadata['coref'])]\n",
    "        data = valid_epochs.get_data()\n",
    "        all_data.append(data)\n",
    "        labels = valid_epochs.metadata\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    full_raw_data = np.concatenate(all_data, axis=0)\n",
    "    metadata = pd.concat(all_labels, axis=0, ignore_index=True)\n",
    "\n",
    "    for pronoun in pronouns:\n",
    "        subset_df = metadata[metadata[\"POS\"] == pronoun]\n",
    "        coref_counts = subset_df[\"coref\"].value_counts()\n",
    "        sig_coref = coref_counts[coref_counts > 30].index.tolist()\n",
    "        sig_chars = [char for char in characters if char in sig_coref]\n",
    "\n",
    "        char_df = metadata[metadata[\"coref\"].isin(sig_chars)]\n",
    "        y = char_df[[\"coref\"]]\n",
    "        X = full_raw_data[char_df.index]\n",
    "\n",
    "        breakpoint()\n",
    "        enc = OneHotEncoder()\n",
    "        enc_y = enc.fit_transform(y)\n",
    "        label_df = pd.DataFrame(enc_y.toarray(), columns=enc.categories_)\n",
    "\n",
    "        pipeline = make_pipeline(StandardScaler(), LogisticRegression(\n",
    "            random_state=125, max_iter=10000, solver=\"lbfgs\", C=10e-3))\n",
    "\n",
    "        df_scores = run_LR_model(X, label_df, pipeline)\n",
    "        df_scores.to_csv(\n",
    "            f'{save_dir}/df_scores_pt_{pat_id}_trial_{pronoun}.csv', index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to run Analysis 2.\n",
    "    \"\"\"\n",
    "    root_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Data\"\n",
    "    save_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Code\\Results\\Analysis_2\"\n",
    "    patients = [\"01\", \"02\", \"03\"]\n",
    "    for patient in patients:\n",
    "        analysis_2(root_dir, save_dir, patient)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Data\"\n",
    "save_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Code\\Results\\Analysis_2\"\n",
    "pat_id = \"01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f\"patient {pat_id}\")\n",
    "    all_data = []\n",
    "    all_labels = []\n",
    "    ses_ids = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "    characters = ['Holmes', 'Watson', 'Doctor', 'McCarthy', 'Clair', 'Doran', \n",
    "                  'Turner', 'Sutherland', 'Simon', 'Ryder', 'Frank', 'Hatherley', \n",
    "                  'Stoner', 'Adler', 'Wilson', 'Ferguson', 'Angel', 'Lestrade']\n",
    "    pronouns = ['I', 'YOU', 'HE', 'MY', 'HIS', 'ME', 'YOUR', 'SHE', 'HIM', 'HER']\n",
    "\n",
    "    for ses_id in ses_ids:\n",
    "        meg_path = f'{root_dir}/sub-0{pat_id}/ses-0{ses_id}/meg/sub-0{pat_id}_ses-0{ses_id}_task-compr_meg.ds'\n",
    "        event_path = f'{root_dir}/sub-0{pat_id}/ses-0{ses_id}/meg/sub-0{pat_id}_ses-0{ses_id}_task-compr_events.tsv'\n",
    "\n",
    "        # raw, df = read_meg(meg_path, event_path)\n",
    "        text = session_text(root_dir, ses_id)\n",
    "\n",
    "        print(f\"Processing session {ses_id} for patient {pat_id}\")\n",
    "\n",
    "        epochs = mne.read_epochs(f\"{root_dir}/sub_0{pat_id}/ses_0{ses_id}/clean-epo.fif\", preload=False)\n",
    "        df = epochs.metadata\n",
    "\n",
    "        doc = nlp(text)\n",
    "        df['POS'], _ = pos_tagger(doc, ses_id, pat_id)\n",
    "        df['coref'] = coref_tagger(doc, ses_id, pat_id)\n",
    "        df[\"syllables\"] = df[\"value\"].apply(syllapy.count)\n",
    "\n",
    "        mask = df[\"POS\"].isin([\"PROPN\", \"PRON\", \"NOUN\"])\n",
    "        noun_epochs = epochs[mask]\n",
    "        valid_epochs = noun_epochs[pd.notnull(noun_epochs.metadata['coref'])]\n",
    "        data = valid_epochs.get_data()\n",
    "        all_data.append(data)\n",
    "        labels = valid_epochs.metadata\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    full_raw_data = np.concatenate(all_data, axis=0)\n",
    "    metadata = pd.concat(all_labels, axis=0, ignore_index=True)\n",
    "\n",
    "    for pronoun in pronouns:\n",
    "        subset_df = metadata[metadata[\"POS\"] == pronoun]\n",
    "        coref_counts = subset_df[\"coref\"].value_counts()\n",
    "        sig_coref = coref_counts[coref_counts > 30].index.tolist()\n",
    "        sig_chars = [char for char in characters if char in sig_coref]\n",
    "\n",
    "        char_df = metadata[metadata[\"coref\"].isin(sig_chars)]\n",
    "        y = char_df[[\"coref\"]]\n",
    "        X = full_raw_data[char_df.index]\n",
    "\n",
    "        enc = OneHotEncoder()\n",
    "        enc_y = enc.fit_transform(y)\n",
    "        label_df = pd.DataFrame(enc_y.toarray(), columns=enc.categories_)\n",
    "\n",
    "        pipeline = make_pipeline(StandardScaler(), LogisticRegression(\n",
    "            random_state=125, max_iter=10000, solver=\"lbfgs\", C=10e-3))\n",
    "\n",
    "        df_scores = run_LR_model(X, label_df, pipeline)\n",
    "        df_scores.to_csv(\n",
    "            f'{save_dir}/df_scores_pt_{pat_id}_trial_{pronoun}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1377, 269, 401)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.concat(all_labels, axis=0, ignore_index=True)\n",
    "\n",
    "for pronoun in pronouns:\n",
    "    subset_df = metadata[metadata[\"value\"] == pronoun]\n",
    "    coref_counts = subset_df[\"coref\"].value_counts()\n",
    "    sig_coref = coref_counts[coref_counts > 30].index.tolist()\n",
    "    sig_chars = [char for char in characters if char in sig_coref]\n",
    "\n",
    "    char_df = metadata[metadata[\"coref\"].isin(sig_chars)]\n",
    "    y = char_df[[\"coref\"]]\n",
    "    X = full_raw_data[char_df.index]\n",
    "\n",
    "    enc = OneHotEncoder()\n",
    "    enc_y = enc.fit_transform(y)\n",
    "    label_df = pd.DataFrame(enc_y.toarray(), columns=enc.categories_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
