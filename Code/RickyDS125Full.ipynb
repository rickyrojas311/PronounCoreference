{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import mne \n",
    "from mne.stats import permutation_cluster_test\n",
    "import sklearn\n",
    "import spacy\n",
    "from fastcoref import spacy_component\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "import syllapy\n",
    "import random\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Decimate by 12 then filter data (1 - 30 Hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.set_log_level(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_meg(meg_path, event_path):\n",
    "    \"\"\"\n",
    "    Read in MEG data from given path, clean and filter returning\n",
    "    dataframe\n",
    "    \"\"\"\n",
    "    raw = mne.io.read_raw_ctf(meg_path, preload=False)\n",
    "    raw.pick(picks=['mag'])\n",
    "    raw.resample(100)\n",
    "    raw.load_data()\n",
    "    raw.filter(0.1, 30, method='iir')\n",
    "    df = pd.read_csv(event_path, delimiter='\\t')\n",
    "    df_crop = df[df['type'].str.contains('word_onset', na=False)]\n",
    "    df_crop = df_crop.query(\"value != 'sp'\")\n",
    "    return raw, df_crop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_text(root_dir, session, pat_id):\n",
    "    full_text = \"\"\n",
    "    if session == \"08\" and pat_id == \"03\":\n",
    "        pattern = re.compile(f'{session}_[1-6]\\.txt')\n",
    "    else:\n",
    "        pattern = re.compile(f'{session}_\\d\\.txt')\n",
    "    for filename in os.listdir(root_dir + \"\\stimuli\"):\n",
    "        if pattern.match(filename):\n",
    "            file_path = os.path.join(root_dir + \"\\stimuli\", filename)\n",
    "            with open(file_path, 'r') as file:\n",
    "                full_text += file.read().replace(\"\\n\", \" \")\n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_accented_characters(token):\n",
    "    for char in token.text:\n",
    "        if ord(char) > 127:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accent(token):\n",
    "    idx = 0\n",
    "    for char in token.text:\n",
    "        if ord(char) > 127:\n",
    "            return idx\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(text, ses_id, pat_id):\n",
    "    \"\"\"\n",
    "    Does not work on words with more then 1 accent\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos = []\n",
    "    tokens = []\n",
    "\n",
    "    for token in doc:\n",
    "        if ses_id == \"01\" and pat_id == \"03\":\n",
    "            if token.i < 10:\n",
    "                continue\n",
    "        if token.text.strip() != \"\" and not token.is_punct:\n",
    "            if has_accented_characters(token):\n",
    "                text = token.text[find_accent(token) + 1:]\n",
    "                if text != \"\":\n",
    "                    pos.append(token.pos_)\n",
    "                    tokens.append(text)\n",
    "            elif token.text.lower() in [\"n't\", \"'ll\", \"'ve\", \"'m\", \"'d\", \"'t\", \"'s\", \"'re\"]:\n",
    "                temp = tokens[-1] + token.text\n",
    "                tokens[-1] = temp\n",
    "            else:\n",
    "                pos.append(token.pos_)\n",
    "                tokens.append(token.text)\n",
    "\n",
    "    return pos, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_epochs(df, raw):\n",
    "    word_samples = np.array(df['onset'] * raw.info['sfreq'], dtype='int')\n",
    "    n_words = len(word_samples)\n",
    "\n",
    "    word_events = np.zeros([n_words, 3], dtype='int')\n",
    "    word_events[:, 0] = word_samples\n",
    "\n",
    "    epochs = mne.Epochs(raw, word_events, tmin=-2.0, tmax=2.0, baseline=(-2.0, 2.0), preload=False, metadata=df)\n",
    "    return epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LR_model(epochs, labels, pipeline):\n",
    "    df_scores = pd.DataFrame()\n",
    "    for column in labels.columns:\n",
    "        column = column[0]\n",
    "        y = labels[column].to_numpy().ravel()\n",
    "        auc_score = []\n",
    "        for i in range(epochs.shape[2]):\n",
    "            X = epochs[:, :, i]\n",
    "            skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "            scores = np.zeros(5)\n",
    "            for j, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "                X_train = X[train_index]\n",
    "                y_train = y[train_index]\n",
    "                X_test = X[test_index]\n",
    "                y_test = y[test_index]\n",
    "                pipeline.fit(X_train, y_train)\n",
    "                score = roc_auc_score(y_test, pipeline.predict_proba(X_test)[:, 1])\n",
    "                scores[j] = score\n",
    "            auc_score.append(scores.mean())\n",
    "        df_scores[column] = auc_score\n",
    "    return df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_1(root_dir, save_dir, ses_ids, pat_id, trial):  \n",
    "     all_data = []\n",
    "     all_labels = []\n",
    "     for ses_id in ses_ids:\n",
    "          characters = ['Holmes', 'Watson', 'Doctor', 'McCarthy', 'Doran', 'Turner', \n",
    "                'Simon', 'Ryder', 'Stoner', 'Adler', 'Wilson', 'Angel', 'Lestrade']\n",
    "          upper_chars = [char.upper() for char in characters]\n",
    "\n",
    "          # raw, df = read_meg(meg_path, event_path)\n",
    "          text = session_text(root_dir, ses_id, pat_id)\n",
    "          # epochs = create_epochs(df, raw)\n",
    "          epochs = mne.read_epochs(f\"{root_dir}/sub_0{pat_id}/ses_0{ses_id}/clean-epo.fif\", preload=False)\n",
    "          df = epochs.metadata\n",
    "\n",
    "          print(ses_id, pat_id)\n",
    "          df['POS'], token_text = pos_tagger(text, ses_id, pat_id)\n",
    "          mask = df[\"POS\"].isin([\"PRON\"]) | (df[\"POS\"].isin([\"PROPN\"]) & df[\"value\"].isin(upper_chars))\n",
    "          noun_epochs = epochs[mask]\n",
    "          data = noun_epochs.get_data()\n",
    "          all_data.append(data)\n",
    "          labels = noun_epochs.metadata[[\"POS\"]]\n",
    "          all_labels.append(labels)\n",
    "\n",
    "     X = np.concatenate(all_data, axis=0)\n",
    "     y = pd.concat(all_labels, axis=0)\n",
    "\n",
    "\n",
    "     enc = OneHotEncoder()\n",
    "     enc_y = enc.fit_transform(X=y)\n",
    "     label_df = pd.DataFrame(enc_y.toarray(), columns=enc.categories_)\n",
    "\n",
    "     pipeline = make_pipeline(StandardScaler(), LogisticRegression(\n",
    "          random_state= 125, max_iter=10000, solver=\"lbfgs\", C=10e-3))\n",
    "\n",
    "\n",
    "     df_scores = run_LR_model(X, label_df, pipeline)\n",
    "     df_scores.to_csv(save_dir + f\"/df_scores_pt_{pat_id}_trial_{trial}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09 01\n",
      "07 01\n",
      "01 01\n",
      "03 01\n",
      "02 01\n",
      "06 01\n",
      "08 01\n",
      "05 01\n",
      "10 01\n",
      "04 01\n",
      "09 02\n",
      "07 02\n",
      "01 02\n",
      "03 02\n",
      "02 02\n",
      "06 02\n",
      "08 02\n",
      "05 02\n",
      "10 02\n",
      "04 02\n",
      "09 03\n",
      "07 03\n",
      "01 03\n",
      "03 03\n",
      "02 03\n",
      "06 03\n",
      "08 03\n",
      "05 03\n",
      "10 03\n",
      "04 03\n",
      "[('09', '07'), ('01', '03'), ('02', '06'), ('08', '05'), ('10', '04')]\n"
     ]
    }
   ],
   "source": [
    "root_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Data\"\n",
    "save_dir = r\"C:\\Users\\ricky\\OneDrive\\Desktop\\Datasci125\\Code\\Results\\Analysis_1C\"\n",
    "\n",
    "sessions = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\"]\n",
    "patients = [\"01\", \"02\", \"03\"]\n",
    "\n",
    "random.seed(125)\n",
    "random.shuffle(sessions)\n",
    "\n",
    "random_pairs = [(sessions[i], sessions[i + 1]) for i in range(0, len(sessions), 2)]\n",
    "\n",
    "# patients = [\"03\"]\n",
    "# random_pairs = [('01', '04'), ('05', '07')]\n",
    "\n",
    "for patient in patients:\n",
    "    for trial, pair in enumerate(random_pairs):\n",
    "        analysis_1(root_dir, save_dir, pair, patient, trial)\n",
    "\n",
    "print(random_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.5 Hours per pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'noun_epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnoun_epochs\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'noun_epochs' is not defined"
     ]
    }
   ],
   "source": [
    "noun_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
